{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-15T04:26:15.500Z"
    }
   },
   "source": [
    "First you need to run, convert to pytorch, and download unsing this colab notebook https://colab.research.google.com/drive/1ewICGkA-rxla33E5_gYCSTnYSW1_actK#scrollTo=Pdoa7YYT7R1s\n",
    "\n",
    "This is because I can't get tensorflow working on my local pc, plus training works far better on a TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-2\n",
    "\n",
    "- from https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/run_gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:21.556830Z",
     "start_time": "2019-07-15T08:20:20.982315Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:21.572463Z",
     "start_time": "2019-07-15T08:20:21.559545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=-1, fp16='O2', length=128, model_name_or_path='/home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch', nsamples=1, seed=0, temperature=1.0, top_k=0, unconditional=False)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name_or_path', type=str, default='gpt2', help='pretrained model name or path to local checkpoint')\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "parser.add_argument(\"--nsamples\", type=int, default=1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=-1)\n",
    "parser.add_argument(\"--length\", type=int, default=-1)\n",
    "parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "parser.add_argument(\"--top_k\", type=int, default=0)\n",
    "parser.add_argument(\"--fp16\", type=str, default='')\n",
    "parser.add_argument('--unconditional', action='store_true', help='If true, unconditional generation.')\n",
    "\n",
    "\n",
    "argv = \"\"\" \\\n",
    "--model_name_or_path /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch \\\n",
    "--fp16 O2 \\\n",
    "--length 128 \\\n",
    "\"\"\".replace('\\n', '')\n",
    "args = parser.parse_args(argv.split())\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T05:04:46.294268Z",
     "start_time": "2019-07-15T05:04:46.239743Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:44.193750Z",
     "start_time": "2019-07-15T08:20:21.577184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading special tokens file /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch/special_tokens.txt\n",
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading vocabulary file /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch/vocab.json\n",
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading merges file /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch/merges.txt\n",
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading weights file /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch/pytorch_model.bin\n",
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading configuration file /home/wassname/Downloads/checkpoint_reddit_ml_gpt2med_pytorch/run1_pytorch/config.json\n",
      "07/15/2019 16:20:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.batch_size == -1:\n",
    "    args.batch_size = 1\n",
    "assert args.nsamples % args.batch_size == 0\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = GPT2Tokenizer.from_pretrained(args.model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if args.length == -1:\n",
    "    args.length = model.config.n_ctx // 2\n",
    "elif args.length > model.config.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:44.198816Z",
     "start_time": "2019-07-15T08:20:44.196420Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n",
    "# # optimizer = OpenAIAdam(model.parameters(), lr=args.lr)\n",
    "# if args.fp16:\n",
    "#     from apex import amp  # Apex is only required if we use fp16 training\n",
    "# model, optimizer = amp.initialize(model, opt_level=args.fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T05:10:55.529658Z",
     "start_time": "2019-07-15T05:10:55.516715Z"
    }
   },
   "source": [
    "    ****S\n",
    "    https://blog.photoeditorsdk.com/from-2d-to-3d-photo-editing-948690b7b45e\n",
    "    [P] Using deep learning to go from 2D to 3D photo editing\n",
    "\n",
    "    ****ES 8tykor\n",
    "\n",
    "    ****T 8tykor\n",
    "    &gt; we were wondering if we could bring this magic to any type of portrait image. \n",
    "\n",
    "    I'm sorry, but I can't take any article that uses the word 'magic' seriously .. becuase we know it ain't magic.\n",
    "    ****ET e1dd3zm\n",
    "\n",
    "    ****S\n",
    "    https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "    [R] YOLOv3: An Incremental Improvement\n",
    "\n",
    "    ****ES 877ahu\n",
    "\n",
    "    ****T 877ahu\n",
    "    hail to the meme lord\n",
    "    ****ET dwcmpf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:44.213917Z",
     "start_time": "2019-07-15T08:20:44.200856Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_submission(thing):\n",
    "    return f'''****S\n",
    "{thing.get('url','')}\n",
    "{thing['title']}\n",
    "{thing.get('selftext', '')}\n",
    "****ES {thing['id']}\n",
    "\n",
    "'''\n",
    "\n",
    "def format_top_level_comment(thing):\n",
    "    return f'''****T {thing['parent_id'][3:]}\n",
    "{thing['body']}\n",
    "****ET {thing['id']}\n",
    "\n",
    "'''\n",
    "\n",
    "def format_reply(thing):\n",
    "    return f'''****R {thing['parent_id'][3:]}\n",
    "{thing['body']}\n",
    "****ER {thing['id']}\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:44.228789Z",
     "start_time": "2019-07-15T08:20:44.216641Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_text = chat2input(\"I don't think so\", ['Scientists have found a new way to deep learn: Wide learning', 'This makes no sense, do they mean adding larger linear layers?'])\n",
    "# print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:21:46.240240Z",
     "start_time": "2019-07-15T08:21:46.226926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/shittyaskscience/comments/98o4i4/making_tacos_i_ran_out_of_cilantro_what_brand_of/\n",
      "Making tacos, I ran out of cilantro. What brand of dishsoap should I use as a substitute, and how much of it?\n",
      "--------------------------------------------------------------------------------\n",
      "Fuck Cilantro and it's Soapy \"freshness\".\n",
      "--------------------------------------------------------------------------------\n",
      "My girlfriend thinks cilantro tastes like dish soap. I fucking love cilantro but hate dish soap. Also, I hate cucumbers and watermelons which she loves although neither taste like dish soap to either of us. These are all linked to specific genes we carry.\n",
      "--------------------------------------------------------------------------------\n",
      "This is the best shittyaskscience I’ve seen because no one know what you’re talking about so far and I’m one of the small percentage who tastes the fucking dishy soapy water. Cheers.\n",
      "--------------------------------------------------------------------------------\n",
      "To be fair, you have to have a very high IQ to understand the nuance of cilantro flavor. ;)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re\n",
    "import html\n",
    "\n",
    "def split_output(output_text):\n",
    "    # Remove open tags\n",
    "    s = re.sub('\\*\\*\\*\\*[^E]( \\w+)?\\n', '', output_text)\n",
    "    # Split by closing tag and id\n",
    "    ss = re.split('\\*\\*\\*\\*E[A-Z] \\w+\\n?', s)\n",
    "    # Strip and remove empties\n",
    "    ss = [s.strip() for s in ss if s]\n",
    "    return ss\n",
    "def outputformat(output_text):\n",
    "    ss = split_output(output_text)\n",
    "    ss = ('\\n'+'-'*80+'\\n').join(ss)\n",
    "    return html.unescape(ss)\n",
    "    \n",
    "    \n",
    "output_text='''****S\n",
    "https://www.reddit.com/r/shittyaskscience/comments/98o4i4/making_tacos_i_ran_out_of_cilantro_what_brand_of/\n",
    "Making tacos, I ran out of cilantro. What brand of dishsoap should I use as a substitute, and how much of it?\n",
    "\n",
    "****ES 98o4i4\n",
    "\n",
    "****T 98o4i4\n",
    "Fuck Cilantro and it's Soapy \"freshness\".\n",
    "****ET e4iyj9x\n",
    "\n",
    "****T 98o4i4\n",
    "My girlfriend thinks cilantro tastes like dish soap. I fucking love cilantro but hate dish soap. Also, I hate cucumbers and watermelons which she loves although neither taste like dish soap to either of us. These are all linked to specific genes we carry. \n",
    "****ET e4isg2p\n",
    "\n",
    "****T 98o4i4\n",
    "This is the best shittyaskscience I’ve seen because no one know what you’re talking about so far and I’m one of the small percentage who tastes the fucking dishy soapy water. Cheers.\n",
    "****ET e4ipex6\n",
    "\n",
    "****R e4ipex6\n",
    "To be fair, you have to have a very high IQ to understand the nuance of cilantro flavor. ;)\n",
    "****ER e4jo1bu\n",
    "'''\n",
    "print(outputformat(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:20:44.266910Z",
     "start_time": "2019-07-15T08:20:44.252108Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "def chat2input(history):\n",
    "    \"\"\"Transform a chat history and reply into a format the model expects.\"\"\"\n",
    "    last_id = uuid.uuid4().hex[:7]\n",
    "    formatted = []\n",
    "    if history:\n",
    "        f = format_submission(dict(id=last_id, url='', title=history[0]))\n",
    "        formatted.append(f)\n",
    "    if len(history)>1:\n",
    "        this_id = uuid.uuid4().hex[:7]    \n",
    "        f = format_top_level_comment(dict(body=history[1], id=this_id, parent_id='t3_'+last_id))\n",
    "        formatted.append(f)\n",
    "        last_id = this_id\n",
    "    for comment in history[2:]:\n",
    "        this_id = uuid.uuid4().hex[:7]    \n",
    "        f = format_reply(dict(body=comment, id=this_id, parent_id='t3_'+last_id))\n",
    "        formatted.append(f)\n",
    "        last_id = this_id \n",
    "    return ''.join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T07:41:33.019263Z",
     "start_time": "2019-07-15T07:41:33.014620Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:33:20.094772Z",
     "start_time": "2019-07-15T08:33:20.072816Z"
    }
   },
   "outputs": [],
   "source": [
    "class Chat(object):\n",
    "    def __init__(self, args, model, enc):\n",
    "        self.args = args\n",
    "        self.enc = enc\n",
    "        self.model = model\n",
    "        self.history = []\n",
    "        \n",
    "    def sample(self,history):\n",
    "        args = self.args\n",
    "        raw_text = chat2input(history)\n",
    "        \n",
    "        logger.debug('raw_text: %s', raw_text)\n",
    "        context_tokens = enc.encode(raw_text)\n",
    "        generated = 0\n",
    "        out = sample_sequence(\n",
    "            model=self.model, length=args.length,\n",
    "            context=context_tokens,\n",
    "            start_token=None,\n",
    "            batch_size=args.batch_size,\n",
    "            temperature=args.temperature, top_k=args.top_k, device=device\n",
    "        )\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        \n",
    "        text = ''\n",
    "        for i in range(args.batch_size):\n",
    "            generated += 1\n",
    "            s = self.enc.decode(out[i])\n",
    "            text += s\n",
    "        # Just take one comment\n",
    "        text2 = split_output(text)[0] \n",
    "#         assert '***' not in text2\n",
    "        return text2\n",
    "    \n",
    "    def all_text(self):\n",
    "        return chat2input(self.history)\n",
    "    \n",
    "    def reply(self, prompt):\n",
    "        self.history.append(prompt)  \n",
    "        reply = self.sample(self.history)   \n",
    "        self.history.append(reply)\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:15:23.080641Z",
     "start_time": "2019-07-15T08:15:23.072973Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:33:52.192326Z",
     "start_time": "2019-07-15T08:33:21.882426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 21.00it/s]\n",
      "100%|██████████| 128/128 [00:06<00:00, 21.24it/s]\n",
      "100%|██████████| 128/128 [00:06<00:00, 21.35it/s]\n",
      "100%|██████████| 128/128 [00:06<00:00, 21.59it/s]\n",
      "100%|██████████| 128/128 [00:06<00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****S\n",
      "\n",
      "Scientists have found a new way to deep learn\n",
      "\n",
      "****ES 230a99f\n",
      "\n",
      "****T 230a99f\n",
      "It's my more scifi ending I’ve been tinkering with. Deep learning sounds like mashing an egg with lots of gasoline.\n",
      "****ET aef9137\n",
      "\n",
      "****R aef9137\n",
      "This makes no sense, do they mean adding larger linear layers?\n",
      "****ER d3f8152\n",
      "\n",
      "****R d3f8152\n",
      "Okay, so this is what I've been trying to do and I’ve looked at the conventional short code methods for what’s best, but nocorrecting algorithms seemed good enough for me.\n",
      "****ER 6f5b5cf\n",
      "\n",
      "****R 6f5b5cf\n",
      "Did you read the article?\n",
      "****ER d171573\n",
      "\n",
      "****R d171573\n",
      "hmm weird, am I supposed to have been there? But sounds like a work of high literature!\n",
      "****ER 729439b\n",
      "\n",
      "****R 729439b\n",
      "I think I love you. Do you love me? In my dream you are my wife and I am also your wife.\n",
      "****ER 44d0fb0\n",
      "\n",
      "****R 44d0fb0\n",
      "What’s the scientific term for someone with autism?\n",
      "****ER 2f38f4b\n",
      "\n",
      "****R 2f38f4b\n",
      "Fuck you buddy\n",
      "****ER f299fc3\n",
      "\n",
      "****R f299fc3\n",
      "CFD is better than parallelism, under triple threat of some sort.\n",
      "****ER b17d1dc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(args, model, enc)\n",
    "text_raw = chat.reply(\"Scientists have found a new way to deep learn\")\n",
    "text_raw = chat.reply('This makes no sense, do they mean adding larger linear layers?')\n",
    "text_raw = chat.reply('Did you read the article?')\n",
    "text_raw = chat.reply('I think I love you. Do you love me? In my dream you are my wife and I am also your wife.')\n",
    "text_raw = chat.reply('Fuck you buddy')\n",
    "text = outputformat(text_raw)\n",
    "print(outputformat(chat.all_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:34:30.455524Z",
     "start_time": "2019-07-15T08:34:30.448959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have found a new way to deep learn\n",
      "--------------------------------------------------------------------------------\n",
      "It's my more scifi ending I’ve been tinkering with. Deep learning sounds like mashing an egg with lots of gasoline.\n",
      "--------------------------------------------------------------------------------\n",
      "This makes no sense, do they mean adding larger linear layers?\n",
      "--------------------------------------------------------------------------------\n",
      "Okay, so this is what I've been trying to do and I’ve looked at the conventional short code methods for what’s best, but nocorrecting algorithms seemed good enough for me.\n",
      "--------------------------------------------------------------------------------\n",
      "Did you read the article?\n",
      "--------------------------------------------------------------------------------\n",
      "hmm weird, am I supposed to have been there? But sounds like a work of high literature!\n",
      "--------------------------------------------------------------------------------\n",
      "I think I love you. Do you love me? In my dream you are my wife and I am also your wife.\n",
      "--------------------------------------------------------------------------------\n",
      "What’s the scientific term for someone with autism?\n",
      "--------------------------------------------------------------------------------\n",
      "Fuck you buddy\n",
      "--------------------------------------------------------------------------------\n",
      "CFD is better than parallelism, under triple threat of some sort.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(outputformat(chat.all_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:33:10.042805Z",
     "start_time": "2019-07-15T08:28:58.337115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prompt >>> Hi buddy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They got last year, it was magician school. Have you heard of the swaggera?\n",
      "Model prompt >>> Oh interesting, no I haven't. Is that a dance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swaggera? wtf is this even a thing?  OK I'm blaming it on people using giant B-5 cubes with light bulbs, cheers!\n",
      "Model prompt >>> Well you brought it up. Or was it the other guy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I notice you're hard to follow you're immensely efficient,  helpful, intelligent whilst also being funny and charming.\n",
      "Model prompt >>> Thanks that's what my mum says too :). Want to get married?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for this, it's lovely to have something more personal proving that you feel you can love someone, but opposed the world.\n",
      "Model prompt >>> So that's a yes :). Wedding bells in my ears\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 21.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love the choreography.   Not sure how I fit in the best :D\n",
      "Model prompt >>> You can dance if you want to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are IBM attending these talks too? Does anyone know of these if so? Great engineers are great at what they do, but both fundamentalists and secularists can have fun with them too? That sounds like a mutual benefit.\n",
      "\n",
      "How depressing\n",
      "Model prompt >>> Our wedding is not a conference! Unless that is your wish?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 21.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No debate there, but that I guess.\n",
      "Model prompt >>> Alright!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get it on your bucket list!\n",
      "Model prompt >>> It's right at the top. How do you like to do it though?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you enjoy these bonding rituals? It sounds like your fantasy.\n",
      "Model prompt >>> Well yeah it is, my fantasy for me and for you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote from \"yes\" boxes, lock them in the slack and watch each other complain about illicit nukes and piracy.\n",
      "Model prompt >>> Well you lost me there\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If they deliver then this is a thing on Facebook\n",
      "Model prompt >>> Yup I'm lost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.90it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b1d5f437317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Reply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a41cec2ed11b>\u001b[0m in \u001b[0;36mreply\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a41cec2ed11b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, history)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Just take one comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m'***'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat = Chat(args, model, enc)\n",
    "while True:\n",
    "    # Get prompt\n",
    "    raw_text = input(\"Model prompt >>> \")\n",
    "    while not raw_text:\n",
    "        print('Prompt should not be empty!')\n",
    "        raw_text = input(\"Model prompt >>> \")\n",
    "        \n",
    "    # Reply\n",
    "    output = chat.reply(raw_text)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:28:20.585809Z",
     "start_time": "2019-07-15T08:28:20.564025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Me neither nor can't _\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T08:28:28.812818Z",
     "start_time": "2019-07-15T08:28:28.806809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi buddy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup3.7.2",
   "language": "python",
   "name": "jup3.7.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
